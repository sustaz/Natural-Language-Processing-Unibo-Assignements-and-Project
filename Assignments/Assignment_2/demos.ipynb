{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos\n",
    "\n",
    "This is a temporary notebook created in order to try various stuff without mess around too much in the assignment code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/enrico/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enrico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of sentences\n",
    "sentences = [\"I ate dinner.\", \n",
    "       \"We had a three-course meal.\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"He loves fish tacos.\",\n",
    "       \"In the end, we all felt like we ate too much.\",\n",
    "       \"We all agreed; it was a magnificent evening.\"]\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing cosine similarity\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner', '.'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal', '.'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'we': 1,\n",
       " 'ate': 2,\n",
       " 'dinner': 3,\n",
       " 'a': 4,\n",
       " 'all': 5,\n",
       " 'evening': 6,\n",
       " 'came': 7,\n",
       " 'us': 8,\n",
       " 'with': 9,\n",
       " 'to': 10,\n",
       " 'three-course': 11,\n",
       " 'brad': 12,\n",
       " 'meal': 13,\n",
       " 'loves': 14,\n",
       " 'had': 15,\n",
       " 'he': 16,\n",
       " 'fish': 17,\n",
       " 'magnificent': 18,\n",
       " 'tacos': 19,\n",
       " 'in': 20,\n",
       " 'the': 21,\n",
       " 'end': 22,\n",
       " ',': 23,\n",
       " 'felt': 24,\n",
       " 'like': 25,\n",
       " 'too': 26,\n",
       " 'much': 27,\n",
       " 'agreed': 28,\n",
       " ';': 29,\n",
       " 'it': 30,\n",
       " 'was': 31,\n",
       " 'i': 32}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "## Print model vocabulary\n",
    "model.wv.key_to_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take up a new test sentence and find the top 5 most similar sentences from our data. We will also display them in order of decreasing similarity. The infer_vector method returns the vectorized form of the test sentence(including the paragraph vector). The most_similar method returns similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.7299023866653442),\n",
       " (3, 0.5823500156402588),\n",
       " (1, 0.5418941378593445),\n",
       " (2, 0.5351356267929077),\n",
       " (5, 0.44702044129371643),\n",
       " (0, 0.2542485296726227)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.dv.most_similar(positive = [test_doc_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
