{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1G5ro3Qgsbx"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Preliminary Steps\n",
        "\n",
        "Let's import all the needed packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ0pMOoQgsb3",
        "outputId": "6935bcb9-adbd-4861-b95c-03c89cd1ebcb"
      },
      "source": [
        "# Handle files and unzip\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Neural Networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Word tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm as tq \n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "import tensorflow as tf "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hKWjQnngsb4"
      },
      "source": [
        "Let's download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdxMyEnXgsb6"
      },
      "source": [
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "    \n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(data_path):\n",
        "    toy_data_path = os.path.join(data_path, 'fever_data.zip')\n",
        "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
        "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        os.makedirs(data_path)\n",
        "\n",
        "    if not os.path.exists(toy_data_path):\n",
        "        print(\"Downloading FEVER data splits...\")\n",
        "        with requests.Session() as current_session:\n",
        "            response = current_session.get(toy_url,\n",
        "                                   params={'id': toy_data_url_id},\n",
        "                                   stream=True)\n",
        "        save_response_content(response, toy_data_path)\n",
        "        print(\"Download completed!\")\n",
        "\n",
        "        print(\"Extracting dataset...\")\n",
        "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
        "            loaded_zip.extractall(data_path)\n",
        "        print(\"Extraction completed!\")\n",
        "\n",
        "download_data('dataset')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXcFrhqOgsb7"
      },
      "source": [
        "Look inside our dataset creating a first dataframe reading the `test_pairs.csv` file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwkhN6Digsb8",
        "outputId": "0ab797e3-e570-465d-cea2-e2b86f84cf1a"
      },
      "source": [
        "test_path = os.path.join(os.getcwd(), 'dataset', 'train_pairs.csv')\n",
        "\n",
        "df0 = pd.read_csv(test_path)\n",
        "print(df0.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                              Claim  ...  ID     Label\n",
            "0           0     Chris Hemsworth appeared in A Perfect Getaway.  ...   3  SUPPORTS\n",
            "1           1                            Roald Dahl is a writer.  ...   7  SUPPORTS\n",
            "2           2                          Roald Dahl is a governor.  ...   8   REFUTES\n",
            "3           3        Ireland has relatively low-lying mountains.  ...   9  SUPPORTS\n",
            "4           4  Ireland does not have relatively low-lying mou...  ...  10   REFUTES\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc5CHrVCgsb-"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba7-IVsOgsb_"
      },
      "source": [
        "As stated in [specifications.ipynb](specifications.ipynb), each dataset sample is comprised of:\n",
        "\n",
        "*     A claim to verify\n",
        "*     A set of semantically related statements (evidence set)\n",
        "*     Fact checking label: either evidences support or refute the claim.\n",
        "\n",
        "Handling the evidence set from the point of view of neural models may imply some additional complexity: if the evidence set is comprised of several sentences we might incur in memory problems.\n",
        "\n",
        "To this end, we further simplify the problem by building (claim, evidence) pairs. The fact checking label is propagated as well.\n",
        "\n",
        "Example:\n",
        "\n",
        "     Claim: c1 \n",
        "     Evidence set: [e1, e2, e3]\n",
        "     Label: S (support)\n",
        "\n",
        "--->\n",
        "\n",
        "    (c1, e1, S),\n",
        "    (c1, e2, S),\n",
        "    (c1, e3, S)\n",
        "\n",
        "So now we construct a new dataframe where each sample is in the form (Claim, Evidence, Lable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMslRQmygsb_"
      },
      "source": [
        "# the evidences seems to be separated by a tab (\\t) \n",
        "# so it may be used to split the Evidence string\n",
        "\n",
        "# this dataset is a mess\n",
        "dataset_path = os.path.join(os.getcwd(), 'dataset', 'train_pairs.csv')\n",
        "\n",
        "def sentence_cleaning(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    cleans up a sentence in the dataset using regular expressions\n",
        "    :param sentence: the sentence to clean-up\n",
        "    :return\n",
        "        - string cleaned\n",
        "    \"\"\"\n",
        "    \n",
        "    # removes \"-LRB-\" and \"-RRB-\" strings and commas\n",
        "    sentence = re.sub(\"-LRB-\",\"\", sentence)\n",
        "    sentence = re.sub(\"-RRB-\", \"\", sentence)\n",
        "    sentence = re.sub(\",\", \"\", sentence)\n",
        "    \n",
        "    return sentence\n",
        "    \n",
        "\n",
        "def format_dataset(dataset: str, debug: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads out the csv file and returns a dataframe with \n",
        "    {Claim, Evidence, Label} row\n",
        "    \n",
        "    :param dataset: dataset csv file path\n",
        "    :param debug: if True prints out data for debugging purposes\n",
        "    \n",
        "    :return\n",
        "        - dataframe with (claim, evidence, label) rows\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_csv(dataset)\n",
        "    dataframe_rows = []\n",
        "    df_size = df.shape[0]\n",
        "\n",
        "    for i in range(0, df_size):\n",
        "\n",
        "        claim = df[\"Claim\"][i]\n",
        "        label = df[\"Label\"][i]\n",
        "        ev_list = df[\"Evidence\"][i].split('\\t')\n",
        "\n",
        "        evidence = ev_list[1]\n",
        "        \n",
        "        # create single dataframe row\n",
        "        dataframe_row = {\n",
        "            \"Claim\": claim,\n",
        "            \"Evidence\": evidence,\n",
        "            \"Label\": label\n",
        "        }\n",
        "\n",
        "        if debug: \n",
        "            print(claim)\n",
        "            print(evidence)\n",
        "            print(label)\n",
        "        dataframe_rows.append(dataframe_row)\n",
        "\n",
        "    df = pd.DataFrame(dataframe_rows)\n",
        "    \n",
        "    return df, dataframe_rows\n",
        "\n",
        "df, df_rows = format_dataset(dataset_path, False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbSeumrRgscB"
      },
      "source": [
        "Let's see how the resulting dataframe looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "evoJpSSygscB",
        "outputId": "c4d34656-4b74-452c-d8c6-5f0731e4a797"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Claim</th>\n",
              "      <th>Evidence</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
              "      <td>Hemsworth has also appeared in the science fic...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Roald Dahl is a writer.</td>\n",
              "      <td>Roald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑː...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Roald Dahl is a governor.</td>\n",
              "      <td>Roald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈdɑː...</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ireland has relatively low-lying mountains.</td>\n",
              "      <td>The island 's geography comprises relatively l...</td>\n",
              "      <td>SUPPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ireland does not have relatively low-lying mou...</td>\n",
              "      <td>The island 's geography comprises relatively l...</td>\n",
              "      <td>REFUTES</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Claim  ...     Label\n",
              "0     Chris Hemsworth appeared in A Perfect Getaway.  ...  SUPPORTS\n",
              "1                            Roald Dahl is a writer.  ...  SUPPORTS\n",
              "2                          Roald Dahl is a governor.  ...   REFUTES\n",
              "3        Ireland has relatively low-lying mountains.  ...  SUPPORTS\n",
              "4  Ireland does not have relatively low-lying mou...  ...   REFUTES\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki23iZ3RgscC"
      },
      "source": [
        "Turning claims and Evidences into sequences of integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ9MhzeWgscC"
      },
      "source": [
        "def make_sequences(texts, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"\n",
        "    Turn a set of texts into sequences of integers\n",
        "    \n",
        "    :param texts: the set of texts to turn into sequences\n",
        "    :param lower: boolean. Whether to convert the texts to lowercase\n",
        "    :param filters: a string where each element is a character that will be filtered from the texts\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "    \n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    \n",
        "    return word_idx, idx_word, num_words, word_counts, sequences\n",
        "\n",
        "def encode_dataframe(dataframe):\n",
        "    \"\"\"\n",
        "    Creates sequences of integers for both Claim and Evidence columns\n",
        "    \"\"\"\n",
        "    \n",
        "    # creates lists of claims and evidences\n",
        "    claims = list(dataframe['Claim'])\n",
        "    evidences = list(dataframe['Evidence'])\n",
        "    \n",
        "    cl_word_idx, cl_idx_word, cl_num_words, cl_word_counts, seq_claims = make_sequences(claims)\n",
        "    ev_word_idx, ev_idx_word, ev_num_words, ev_word_counts, seq_evidences = make_sequences(evidences)\n",
        "    \n",
        "    return seq_claims, seq_evidences\n",
        "\n",
        "seq_claims, seq_evidences = encode_dataframe(df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRtbSu2DgscD",
        "outputId": "e4a05ffa-046d-42d4-d995-83a3b581f106"
      },
      "source": [
        "n = 6\n",
        "\n",
        "print(seq_claims[n])\n",
        "print(len(seq_claims))\n",
        "\n",
        "seq_claims[6][6]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[641, 196, 6, 151, 7337, 13, 2, 135, 5669, 4, 12729]\n",
            "121740\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5RwC0N0gscD"
      },
      "source": [
        "Constructing the input matrices for the embedding step. We construct the matrices starting from `seq_claims` and `seq_evidences`. The number of rows of the matrices is the number of elements in `seq_claims` and `seq_evidences`, while the number of columns is the size of the longest sequences. The rows corresponding to shorter sequences are filled with zero-padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBxP5T4ogscE",
        "outputId": "59f9b1e6-691a-402f-ff5e-202d6b46ad0a"
      },
      "source": [
        "# computes the longest token sequence\n",
        "def longest_seq(seq):\n",
        "    seq_len = [len(i) for i in seq]\n",
        "    \n",
        "    return max(seq_len)\n",
        "\n",
        "def matrix_from_sequences(sequences):\n",
        "    \"\"\"\n",
        "    Builds a matrix of shape [batch_size, max_tokens]\n",
        "    \"\"\"\n",
        "    \n",
        "    max_tokens = longest_seq(sequences)\n",
        "    seq_length = len(sequences)\n",
        "    \n",
        "    matrix = np.zeros((len(sequences), max_tokens), dtype=np.int32)\n",
        "    \n",
        "    for i in range(0, seq_length):\n",
        "        for j in range(0, len(sequences[i])):\n",
        "            matrix[i][j] = sequences[i][j]\n",
        "    \n",
        "    return matrix\n",
        "\n",
        "claim_matrix = matrix_from_sequences(seq_claims)\n",
        "evidence_matrix = matrix_from_sequences(seq_evidences)\n",
        "\n",
        "print(claim_matrix.shape)\n",
        "print(evidence_matrix.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(121740, 65)\n",
            "(121740, 126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l6KcdDsgscE",
        "outputId": "e12d057c-bdbf-4071-cd4c-1b73729f7d27"
      },
      "source": [
        "len(claim_matrix[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6WTfYe8gscF"
      },
      "source": [
        "I dunno if this is correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcrImldDgscG"
      },
      "source": [
        "### Sentence Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bisp46eayDHl"
      },
      "source": [
        "Let's reshape the claim and evidence matrixes into [batch_size, sequence_length, embedding_dim]. The embedding is created with keras, involving the similarity of the phrases. Check on https://keras.io/api/layers/core_layers/embedding/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFgmc5KzvLRx"
      },
      "source": [
        "def sentence_embedding(matrix, input_dim, output_dim, input_length):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length))\n",
        "\n",
        "  model.compile('rmsprop', 'mse')\n",
        "  reshaped_matrix = model.predict(matrix)\n",
        "  return reshaped_matrix, model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgYXJOCxw1jS"
      },
      "source": [
        "reshaped_claim_matrix , embed_claim = sentence_embedding(claim_matrix, 121740+1, 50, 65)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUbkO228xvnm"
      },
      "source": [
        "reshaped_evidence_matrix , embed_evidence = sentence_embedding(evidence_matrix, 121740+1, 50, 126)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZdfe0_etvfB",
        "outputId": "c43b60b8-a487-4cfa-def0-77d83fe29ebe"
      },
      "source": [
        "embed_claim.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 65, 50)            6087050   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,087,050\n",
            "Trainable params: 6,087,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}