{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos\n",
    "\n",
    "This is a temporary notebook created in order to try various stuff without mess around too much in the assignment code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/enrico/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in /Users/enrico/anaconda3/lib/python3.8/site-packages (from gensim) (1.18.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/enrico/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U gensim\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of sentences\n",
    "sentences = [\"I ate dinner.\", \n",
    "       \"We had a three-course meal.\", \n",
    "       \"Brad came to dinner with us.\",\n",
    "       \"He loves fish tacos.\",\n",
    "       \"In the end, we all felt like we ate too much.\",\n",
    "       \"We all agreed; it was a magnificent evening.\"]\n",
    "\n",
    "# Tokenization of each document\n",
    "tokenized_sent = []\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for computing cosine similarity\n",
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['i', 'ate', 'dinner', '.'], tags=[0]),\n",
       " TaggedDocument(words=['we', 'had', 'a', 'three-course', 'meal', '.'], tags=[1]),\n",
       " TaggedDocument(words=['brad', 'came', 'to', 'dinner', 'with', 'us', '.'], tags=[2]),\n",
       " TaggedDocument(words=['he', 'loves', 'fish', 'tacos', '.'], tags=[3]),\n",
       " TaggedDocument(words=['in', 'the', 'end', ',', 'we', 'all', 'felt', 'like', 'we', 'ate', 'too', 'much', '.'], tags=[4]),\n",
       " TaggedDocument(words=['we', 'all', 'agreed', ';', 'it', 'was', 'a', 'magnificent', 'evening', '.'], tags=[5])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'we': 1,\n",
       " 'ate': 2,\n",
       " 'dinner': 3,\n",
       " 'a': 4,\n",
       " 'all': 5,\n",
       " 'evening': 6,\n",
       " 'came': 7,\n",
       " 'us': 8,\n",
       " 'with': 9,\n",
       " 'to': 10,\n",
       " 'three-course': 11,\n",
       " 'brad': 12,\n",
       " 'meal': 13,\n",
       " 'loves': 14,\n",
       " 'had': 15,\n",
       " 'he': 16,\n",
       " 'fish': 17,\n",
       " 'magnificent': 18,\n",
       " 'tacos': 19,\n",
       " 'in': 20,\n",
       " 'the': 21,\n",
       " 'end': 22,\n",
       " ',': 23,\n",
       " 'felt': 24,\n",
       " 'like': 25,\n",
       " 'too': 26,\n",
       " 'much': 27,\n",
       " 'agreed': 28,\n",
       " ';': 29,\n",
       " 'it': 30,\n",
       " 'was': 31,\n",
       " 'i': 32}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "## Print model vocabulary\n",
    "model.wv.key_to_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take up a new test sentence and find the top 5 most similar sentences from our data. We will also display them in order of decreasing similarity. The infer_vector method returns the vectorized form of the test sentence(including the paragraph vector). The most_similar method returns similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.7299023866653442),\n",
       " (3, 0.5823500156402588),\n",
       " (1, 0.5418941378593445),\n",
       " (2, 0.5351356267929077),\n",
       " (5, 0.44702044129371643),\n",
       " (0, 0.2542485296726227)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = word_tokenize(\"I had pizza and pasta\".lower())\n",
    "test_doc_vector = model.infer_vector(test_doc)\n",
    "model.dv.most_similar(positive = [test_doc_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "From this article: https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470 i've found a series of notebooks with some stuff on RNN\n",
    "\n",
    "These functions below might be useful (maybe) for creating sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
    "             lower=False):\n",
    "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
    "    \n",
    "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
    "    \n",
    "    # cleans sentences\n",
    "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
    "    \n",
    "    # create sequences of integers from texts\n",
    "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
    "        abstracts, training_len, lower, filters)\n",
    "    \n",
    "    # create train and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
    "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
    "                     'y_train': y_train, 'y_valid': y_valid}\n",
    "    return training_dict, word_idx, idx_word, sequences\n",
    "\n",
    "def make_sequences(texts, training_length = 50,\n",
    "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
    "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
    "    \n",
    "    # Create the tokenizer object and train on texts\n",
    "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    # Create look-up dictionaries and reverse look-ups\n",
    "    word_idx = tokenizer.word_index\n",
    "    idx_word = tokenizer.index_word\n",
    "    num_words = len(word_idx) + 1\n",
    "    word_counts = tokenizer.word_counts\n",
    "    \n",
    "    print(f'There are {num_words} unique words.')\n",
    "    \n",
    "    # Convert text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    # Limit to sequences with more than training length tokens\n",
    "    seq_lengths = [len(x) for x in sequences]\n",
    "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
    "    \n",
    "    new_texts = []\n",
    "    new_sequences = []\n",
    "    \n",
    "    # Only keep sequences with more than training length tokens\n",
    "    for i in over_idx:\n",
    "        new_texts.append(texts[i])\n",
    "        new_sequences.append(sequences[i])\n",
    "        \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through the sequences of tokens\n",
    "    for seq in new_sequences:\n",
    "        \n",
    "        # Create multiple training examples from each sequence\n",
    "        for i in range(training_length, len(seq)):\n",
    "            # Extract the features and label\n",
    "            extract = seq[i - training_length: i + 1]\n",
    "            \n",
    "            # Set the features and label\n",
    "            features.append(extract[:-1])\n",
    "            labels.append(extract[-1])\n",
    "    \n",
    "    print(f'There are {len(features)} sequences.')\n",
    "    \n",
    "    # Return everything needed for setting up the model\n",
    "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
